#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
os.environ['MKL_NUM_THREADS'] = '1'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['OPENBLAS_NUM_THREADS'] = '1'

import sys
import argparse

from nmtpytorch import logger
from nmtpytorch.config import Options
from nmtpytorch.utils.misc import setup_experiment, fix_seed
from nmtpytorch.utils.gpu import GPUManager


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        prog='nmtpy',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description="nmtpy trains/translates/tests a given "
                    "configuration/checkpoint/snapshot",
        argument_default=argparse.SUPPRESS)

    subparsers = parser.add_subparsers(dest='cmd', title='sub-commands',
                                       description='Valid sub-commands')

    # train command
    parser_train = subparsers.add_parser('train', help='train help')
    parser_train.add_argument('-C', '--config', type=str, required=True,
                              help="Experiment configuration file")
    parser_train.add_argument('-s', '--suffix', type=str, default="",
                              help="Optional experiment suffix.")
    parser_train.add_argument('-S', '--short', action='store_true',
                              help="Use short experiment id in filenames.")
    parser_train.add_argument('overrides', nargs="*", default=[],
                              help="(section).key:value overrides for config")

    ###################
    # translate command
    ###################
    parser_trans = subparsers.add_parser('translate', help='translate help')
    parser_trans.add_argument('-n', '--disable-filters', action='store_true',
                              help='Disable eval_filters given in config')
    parser_trans.add_argument('-b', '--batch-size', type=int, default=16,
                              help='Batch size for beam-search')
    parser_trans.add_argument('-k', '--beam-size', type=int, default=6,
                              help='Beam size for beam-search')
    parser_trans.add_argument('-m', '--max-len', type=int, default=200,
                              help='Maximum seq. limit (Default: 200)')
    parser_trans.add_argument('-a', '--lp-alpha', type=float, default=0.,
                              help='Apply length-penalty (Default: 0.)')
    parser_trans.add_argument('-d', '--device-id', type=str, default='auto_1',
                              help='Select GPU device(s)')
    parser_trans.add_argument('models', type=str, nargs='+',
                              help="Saved model/checkpoint file(s)")

    group = parser_trans.add_mutually_exclusive_group(required=True)
    # You can translate a set of splits defined in the .conf file
    group.add_argument('-s', '--splits', type=str,
                       help='Comma separated splits from config file')
    # Or you can provide another input configuration with -S
    group.add_argument('-S', '--source', type=str,
                       help='Comma-separated key:value pairs to provide new inputs.')
    parser_trans.add_argument('-o', '--output', type=str, required=True,
                              help='Output filename prefix')

    ##############
    # test command
    ##############
    parser_test = subparsers.add_parser('test', help='test help')
    parser_test.add_argument('-b', '--batch-size', type=int, default=64,
                              help='Batch size for beam-search')
    parser_test.add_argument('-d', '--device-id', type=str, default='auto_1',
                              help='Select GPU device(s)')
    parser_test.add_argument('models', type=str, nargs='+',
                              help="Saved model/checkpoint file(s)")

    group = parser_test.add_mutually_exclusive_group(required=True)
    group.add_argument('-s', '--splits', type=str,
                       help='Comma separated splits from config file')
    group.add_argument('-S', '--source', type=str,
                       help='Comma-separated key:value pairs to provide new inputs.')

    # Parse command-line arguments first
    args = parser.parse_args()
    if args.cmd is None:
        parser.print_help()
        sys.exit(1)

    # Mode selection
    if args.cmd == 'train':
        # Parse configuration file and merge with the rest
        opts = Options(args.config, args.overrides)

        # Setup experiment folders
        setup_experiment(opts, args.suffix, args.short)

    # Detect device_id
    device_id = (opts.train['device_id'] if args.cmd not in
                 ('test', 'translate') else args.device_id)

    # Reserve GPUs
    gpu_devs = GPUManager()(device_id, strict=True)
    assert len(gpu_devs) == 1, "Multi-GPU mode not implemented yet."

    # translate entry point
    if args.cmd == 'translate':
        from nmtpytorch.translator import Translator
        fix_seed(1234)
        del args.__dict__['cmd']
        translator = Translator(**args.__dict__)
        translator()
        sys.exit(0)
    # test entry point
    elif args.cmd == 'test':
        from nmtpytorch.tester import Tester
        fix_seed(1234)
        del args.__dict__['cmd']
        tester = Tester(**args.__dict__)
        tester()
        sys.exit(0)

    #################################
    # Training / Resuming entry point
    #################################
    import torch
    import platform
    from nmtpytorch import models
    from nmtpytorch.mainloop import MainLoop
    log = logger.setup(opts.train, args.cmd)

    # If given, seed that; if not generate a random seed and print it
    if opts.train['seed'] > 0:
        seed = fix_seed(opts.train['seed'])
    else:
        opts.train['seed'] = fix_seed()

    # Be verbose and fire the loop!
    opts.info(log)

    # Instantiate the model object
    model = getattr(models, opts.train['model_type'])(opts=opts, logger=log)

    log.info("PyTorch {} (CUDA: {}) on '{}' (GPUs: {})".format(
        torch.__version__, torch.version.cuda, platform.node(), gpu_devs))
    log.info("Seed for further reproducibility: {}".format(opts.train['seed']))
    if 'SLURM_JOB_ID' in os.environ:
        log.info("SLURM Job ID: {}".format(os.environ['SLURM_JOB_ID']))
    loop = MainLoop(model, log, opts.train)
    loop()
    sys.exit(0)
