# Sample configuration file

###################################
# Note about variable intrapolation
###################################
# A variable "var" in a "section" can be referenced as ${var} from the
# same section and as ${section:var} from other sections.

###################################################
# Arguments common to training and mainloop.
# The defaults are defined in nmtpytorch/config.py
###################################################

[train]
# auto_N: Select and lock N free GPUs
#      N: Use Nth GPU
#  0,1,N: Use GPUs numbered 0,1 and N
# (Multi-GPU options are there but not implemented yet in
#  training logic.)
device_id: auto_1

# Print info to screen and log file each `disp_freq` minibatches
disp_freq: 30

# If > 0, the seed will be fixed for reproducibility
seed: 0

# A `model_type` should be the class name of the model you'd like to train
# See nmtpytorch/models/
model_type: NMT

# After this many validations without improvement, the training will stop.
patience: 20

# The training will stop after this many epochs
max_epochs: 100

# Same as above but in terms of mini-batch count
max_iterations: 1000000

# An evaluation on held-out `val_set` will be performed
# after each `eval_freq` minibatches
eval_freq: 1000

# Evaluate on validation set once before starting training
eval_zero: False

# Validation warmup. No periodic evaluation
# will be performed before epoch `eval_start` is reached.
eval_start: 1

# One or many evaluation metrics for held-out evaluation
# Early-stopping criterion is always the first one
eval_metrics: meteor,bleu,loss

# Post-processing filters to apply to beam-search outputs and references
# in order to correctly compute metrics (check: nmtpytorch/filters.py)
eval_filters: de-bpe

# Beam size during evaluation
eval_beam: 12

# Batch size for batched beam-search on GPU
eval_batch_size: 12

# Save the best model w.r.t each metric provided in `eval_metrics`
save_best_metrics: True

# Saves a snapshot every `checkpoint_freq` minibatches
checkpoint_freq: 3000

# Keeps a rolling buffer of `n_checkpoints` for periodic checkpointing
n_checkpoints: 4

# Scaling factor for L2 regularization
l2_reg: 1e-5

# Gradient clipping norm
gclip: 5

# Optimizers from PyTorch (in lowercase)
optimizer: adam

# Initial learning rate for the above optimizer (0 uses PyTorch' defaults)
lr: 0.0004

# Training batch_size. Same is used for evaluation loss batching.
batch_size: 64

# Where to save the models
save_path: ~/models

# If given and TensorbardX is installed, TensorBoard files will be
# stored under this folder.
tensorboard_dir: ${save_path}/tb_dir

##################################################################
# Below section is completely dependent on the model_type selected
# The defaults for these arguments are defined in the relevant
# nmtpytorch/models/<model.py>
##################################################################

[model]
# type of attention: mlp/dot (dot is untested)
att_type: mlp

# number of encoder layers
n_encoders: 1

# If n_encoders > 1, defines the dropout between encoder layers
dropout_enc: 0

# Encoder is a bi-directional GRU with enc_dim units
enc_dim: 256

# Decoder is a 2-layer GRU (CGRU) with dec_dim units
dec_dim: 256

# CGRU decoder initialization
#   mean_ctx: h_0 = W_decinit.dot(tanh(mean(encoder states)))
#       zero: h_0 = 0
dec_init: mean_ctx

# Both source/target embeddings are <emb_dim>-dimensional
emb_dim: 128

# 2-way: Shares input/output embeddings of decoder i.e. target-side
# 3-way: 2-way + source embedding sharing
#        this requires **same** vocabularies
#        Check -s argument of nmtpy-build-vocab
# False: disabled.
tied_emb: 2way

# Simple dropout after source embeddings
dropout_emb: 0.3

# Simple dropout top source encoder states
dropout_ctx: 0.5

# Simple dropout before target softmax
dropout_out: 0.5

# Ignores sentences where target side length > 80 words
# Because of how the BPTT is handled by PyTorch, currently we
# have to limit this otherwise we hit GPU memory errors
max_trg_len: 80

# Trains an NMT from en->de
# This automatically take cares of determining src->trg order for
# the below data files. You can just change this to de->en to train
# another NMT for the inverse direction.
direction: en->de

#############################
# Here we define the datasets
#############################
[data]
# A placeholder for data root
root: ~/data/multi30k

# For the `*_set` named variables, here are the general rules:
#   The variables should always have the `_set` suffix.
#   The keys are language IDs and the values are filenames.
#   The values can be globs like *.en for models/datasets which support
#     multiple data files.
#   More than 2 languages/keys can be given. Only keys defined above
#   in the [model]'s `direction` variable will be considered.

# This variable should always be named as **train_set**
train_set: {'en': '${root}/train.lc.norm.tok.bpe.en',
            'de': '${root}/train.lc.norm.tok.bpe.de'}

# The held-out test set on which early-stopping evaluations
# will be performed.
# NOTE that the variable should always be named as **val_set**
val_set: {'en': '${root}/val.lc.norm.tok.bpe.en',
          'de': '${root}/val.lc.norm.tok.bpe.de'}

# It is convenient to define all others test sets you may want to
# translate here. You can then call:
#   nmtpy translate -s test_2016_flickr,test_2017_flickr,val -o output
# to translate all of them without hassle.
test_2016_flickr_set: {'en': '${root}/test_2016_flickr.lc.norm.tok.bpe.en',
                       'de': '${root}/test_2016_flickr.lc.norm.tok.bpe.de'}

test_2017_flickr_set: {'en': '${root}/test_2017_flickr.lc.norm.tok.bpe.en',
                       'de': '${root}/test_2017_flickr.lc.norm.tok.bpe.de'}

###############################################
# Vocabulary files created by nmtpy-build-vocab
# one per each language key
###############################################
[vocabulary]
en: ${data:root}/train.lc.norm.tok.bpe.vocab.en
de: ${data:root}/train.lc.norm.tok.bpe.vocab.de
